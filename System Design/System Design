Load Balancing :
- Helps to spread traffic
- Helps to spread traffic across a cluster of server to improve responsivness and availablity of application or websites
- it keeps track of all resources
- If server is not available LB will stop sending traffic to that server
- Manage the scale
 - Balances the load
 - It helps me scale horizontally
 Eg : 
 Nginx Plus -> 80% market
 Aws Elastic load balancer


 Where should load balancer should be placed
1. Between user and web server
2. Between web server and application server or cache server
3. Betweeen Internal platform / Application /cache to database


 Client-> LB -> webserver1 ---> Application server -> LB -> DB server

Benifits Load Balancer :

1. User experiance faster , Uninterrupted service
Request are moved  
2. Less downtime and higher throughput
3. Handle incoming request while reducing waiting time
4. Smart load balancing :
	Predictive analyiss
Determines traffic bottleneck before they happen

Load balancing algorithm 
1. Least Connection method
2. Least response time method
3. Round robin method
4. Weighted round robin
5. IP hashing
6. Least bandwidth method


Caching :
eg :
Redis,
MemCache
Hazelcast

- LB helps you to scale horizontally across the increasing number of servers
- caching helps to make better use of resoruces you already have
- Response very quickly

Principle of Caching :

Locality of reference
- Recently requested data is likely to be request again.

Lookup in cache can be 1000 times faster than lookup in server

Client browser -> cache -> web server
	site

- used in almost all layer of computing hardware,operating system,web browsers,web applications etc
- cache is short term memory

Snapchat : 


Application Server Cache :


Cache hit :
- Finds the response for the request in the cache layer
- No lookup in database
- quick response time

Cache miss :
- Look in cache -> Not Found : Cache miss
- Go to datbase fetch response
- put it in the cache so next time can be directly fetched from cache


Cache InValidation :

Request -> cache -> Application Database
- Cache must be consistent with database
- If the data has been modified in the db it should be invalidated in cache and updated with new values


Source of Truth : Database NOT cache

This is called cache invalidation
3 ways to achieve cache invalidation

1. Write through cache
- Data is written into the cache and correspoding database at the same time
- cached data allows for fast retrieval since same data gets written in DB
- Data consistency between db and cache
- Any crash,system disruptions , cache is always updated

DisAdvantage :

- Minimise risk of data loss because every write operation must be done twice (DB + cache)
- Higher latency for write operations

Eg : Twitter 100:1 R/W ratio read heavy Application 


2. Write Around Cache
- Similar to write through cache but data is written directly to DB , bypassing the cache
- reduce the cache being flooded with write operation
- 

DisAdvantage :
In case of miss , Read from DB, Re-write into cache and then return response
- Higher latency

3. Write Back cache
- Data is written to cache alone and completion is immediately confirmed to the lcinet
- The write to DB is done after specified interval eg. 30 second
- Low latency for write intensive applications
- Risk of data loss in case of crash because is only in cache


Batch Profile in banking system or Audit system


Cache Eviction policies
1. FIFO
2. LIFO
3. LRU
4. MRU
5. LFU


Content Distribution/ Delivery Network

- CDN are kind of cache which comes into play for serving large amount of static media.
- CDN are usually distributed across different geolocations
eg : 
Netflix -> movie -> CDN server nearest -> Fetch response from there
Netflix -> own CDN -> 2015 openstack


Whats app application 

Sports even / Political event
--> 1 Image / Poster / video ->Circulated a lot
client -> server -> db

Media uploaded to s3 with hash -> CDN

 
 When not to use CDN
 - If system is not large enough
 - keeep static data in seperate 


 Databases :

 1. SQL - Relational DB :
 - Strucuted and pre-defined schema
- Relational DB store data in rows and columns
- Rows = records columns - feilds
- Every row has the same structure
eg, mysql,sql server etc


2. NOSQL
Non-Relational DB
 - no predefined scehama

 1. Key value store
 Data is stored in a array of key-value pair
 Eg : JSON format
 Redis,mongodb,dynamo,voldemort

2. Document Database :
- Data is stored in documents
- Documents are collected together as documents

Wg : Couch DB

3. Wide- Column Database
- instead of tables use columns as containers for rows
- best use for large data size

eg : cassandraand HBase

4. Graph Database

- database where relationship are best represented in graphs
- Data is saved in graph structures with nodes and vertices
eg : Neo4j,InfiniteGraph




CAP theorem :
It is impossible for a distributed software system to simultaneously provide more than two out of theree of the CAP.
Consistency,Availability and partition tolerance

Consistency :
- All Nodes / Instances see the exact same data at the same time
- Constistency is achieved by updating all nodes before allowing further reads

Eg : Banking segment

Availabilty :
- Every request gets a success / failure
- Availability is acheived by having your system up 24*7

Eg. Facebook

Partition tolerance
- system continuous to work despite data loss or partial failure
- A system that is partition -tolerant can sustain any amount of network failure that doesnt result in a failure of entire network
- Data is sufficiently replicated acrpss combination of nodes to keep the system up from intermittent outages

Insta -> Monolithic



Polling :

- Basic client server architecture involves slient sending http request and server responses
- Polling is a technique by which client asks the server for new data regularly

1. Short polling / AJAX
- makes request after fix delay / time
- server cann respond in 2 ways
	1. If a new data is available : sends response as json
	2. If no new data is available : send empty response
- as soon as client received a response it will repeat the above process

Challenges:

2. Long polling :
- server cann respond in 2 ways
	1. If a new data is available : sends response as json
	2. If no new data is available : keep the connection open for period of time and when it received new data then it iwll response back with updated data.

- every request has a certain timeout

NodeJs

Eg. A busy server that is using long polling may have thousands of waiting request and this TCP connections open,Nodejs which makes it easy to manage many connections without creating overhead on systm

Server 
Re timeout : 10 sec
req : 10:00:00 -> response 1 : 10:00:09
req : 10:00:05 -> response 1 : 10:00:06

Challenge :

Message ordering is not guaranteed as you can see above if same client opens multiple connections to server
- Message loss

3. Web Sockets
- Its an communicating protocol that provides full duplex communication channels over a single TCP connection
Client -> server = TCP
server -> client TCP
- websockets enable iterction between a client and a server with less overheads
- provdies rela time transfers from and to the server
- websockets keep the connection open , allowing messages to send and tfrom betwenn client and serve
- 2 way active connection between client and server

Advantages
- keep unique connection while eliminating latency porblem in long polling
- Long polling is much more resoruce intensive because multiple connection open
- websockets are lightweight on server
- Used in real time application like whats app

4. Server side events

- Under sse client will establish a persistent and long term connection with server
- servers uess this connection to sedn data to client
- Client wants to send data to server it would require other protocol eg .tcp cannot vai SSE
- just for notication we can use SSE


Applications of SSE
- Best data delivery
- Real time stock market application
- Real time coverage of new / tweet / links
- Live dashboards for your syste

However SSE is not a viable aleteranative for fast develiery
MMO Massive multiplayer online games needs huge amount of messages from both ends to connection
If latency is at most important real time application stock 

websockets ~= SSE


Data Partitioning : 

- Techniqueue to breakup a big database into smaller parts
- Process of splitting up database across multiple machines.

Justification :
- After certain scale, it is more feasible to scale horizontally
 

 Partitioning Methods
 1. Horizontal Partitioning
 - Put different rows into different tables
 - 
 Eg. Storing locations in DB with zip code

 SNO Name contact ZIP code

 ! million rows

 -Zip code is one of my columns
 Partition :
 ZIP code > 0 and <1000 : Table 1 - Machine 1
 ZIP code > 1000 and <2000 : Table 2 - Machine 2

Disadvantages :
-If wrong partitioning logic is used it will lead to unevenly distrituted data


2. Verticle Partitining

_divide our databased tabled related to a specific feature in their own server

EG: Builiding Insagram
- User Data - DB 1
- Photo they upload - DB 2
- People follow - DB 3


DisAdvantages :

- For additional application users , it might require further partitioning 
Eg.
1 Bn users -> 10 bn pics : cannot be stored in 1 DB instances

3. Directory Based partitioning

- Loosely couple approach to solve the issue from veritical and horizonal partitioning
- create a lookup servicces which knows your current data partitioning and provides much easier access to data.

- To find a data in db -> Directory service-
key : Tuple key,Value : DB server


In veritical scaling 
if data for uuid 123 stored in DB-189
Firstly lookup in directory services -> go to db - 189



Partioning Criteria

1. Key Based / Hash based partitioning
- Apply a hash function to some key attributes to give a partition number
- store data in corresponding database with partition number

we have 100 DB servers
S_NO : Numeric value
Everytime new data is inserted S_no is increased by 1

Hash function S_NO %100 
ADV :
Evenly distributed data

Advantages:
- evenly distributed data

DisAdvantage :
- If data becomes very large such that any 1 instance is NOT able to accomodate data
Add new server -> Change the hash function ->S_np % 101
- Migrate all data -> Include downtime of service

-if data comes very large then new DB needs to added hash function needs to be change from 100 to 101
- Migrate all data - Include downtime of service
- Adding or removing data becomes vary difficult


2. List Partitioning 
- Each DB partition is assigned a list of values
- whenver you want to insert a new record, we will see which partition contains our key and then store in that partition
- 

Eg : Data of all countries

List partition : shengan counties [ frannce,germany,itely,swiss etc]
List partition : Asia counties [ India,shrilanka,pak etc]

Records 

1 India GDP FY
2 Swiss GDP FY

3. Round Robin Partitioning

- Uniform data distibution across instance
- With N partitions ith row is assigned to (i%N) partition

4. Composite / Hybrid partitioning
-Combine any of above partitioining method



Problems of Data partitioniing
- Operation(Queries)) across multiple tables or multiple rows in same table across different servers

1. Joins and Denormalization
	- Performing joinns across multiple tables in 1 instance -> easier
	- Performing joins across multiple tables in multiple instances -> Difficult

	-Such joins will not be performance efficient
	WHy ? Data needs to be compiled from multiple server

	Work-around 
	Denormalize the database so that queries can be performed in single table

Overhead of denormalization :
- Data Inconsistency

2. referencial Integrity
- Performning a cross partitioning query on partioned datbased is not easy
- Includes enforce integrity constants as foregion key in a partitioned database can be different

Most of the dbms do not support forein key constraints across partitioned database
- Workarodund : change in application code
Often application need to run a regular SQL job to clean up dangling reference

3. Rebalancing
- There can be many reasons to rebalance our partitinged scheme

- Too much load on 1 partitione
- uneven distribution
Either we need to add or remove instances 
- Doing this without any downtime is difficult
- If scheme like directory based partitioning makes rebalncing much more difficult because it can create single point of failure(Lookup service)



Indexing :

- DB indexing is used in SQL as well as NOSQL
- B trees data strucutre is mostly used in indexing because logn for all insert delete update
- DB indexing allows use to cut down number of records / rows that needs to be searched when a query is executed

Eg : 
Query :

select * from table where name = 'dev';
every single row is checked to see if name = "dev"
whole table is scanned -> full table scan

An index is data strucutre that stores the value for a certian specific column of table and help us avoid a full table scan.

Eg : Indexing a book
Book pages : 1000 recorsd
chaper 1 : page 1- 50
chapter 2 : page 51-100

Create index on 1 column
Code:
Create index nameindex on table (column1,column2)

Index should be created on table if the data in the indexed column is queries frequently

How indexing works?


Data Structure used 

How do indexes decrease performance :
POST / PUT calls needs to update the index
 